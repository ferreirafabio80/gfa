{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import copy\n",
    "import argparse\n",
    "import visualization_syntdata\n",
    "from models import GFA_DiagonalNoiseModel, GFA_OriginalModel\n",
    "from utils import GFAtools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(args, infoMiss=False):\n",
    "\n",
    "    \"\"\" \n",
    "    Find the most relevant factors.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : Outputs of the model.\n",
    "\n",
    "    res_dir : string\n",
    "        Path to the directory where the results will be saved.   \n",
    "    \n",
    "    BestModel : bool, default to False\n",
    "        Save results of the best model\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    relfactors_shared : list\n",
    "        A list of the relevant shared factors.\n",
    "\n",
    "    relfactors_specific : list\n",
    "        A list of the relevant factors specific to each group.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Generate some data from the generative model, with pre-specified\n",
    "    # latent components\n",
    "    \n",
    "    Ntrain = 400; Ntest = 100\n",
    "    N = Ntrain + Ntest #number of samples\n",
    "    M = args.num_sources  #number of groups/data sources\n",
    "    d = np.array([50, 30]) #number of dimensios for each group\n",
    "    true_K = 4  # true latent components\n",
    "    #Manually specify Z \n",
    "    Z = np.zeros((N, true_K))\n",
    "    for i in range(0, N):\n",
    "        Z[i,0] = np.sin((i+1)/(N/20))\n",
    "        Z[i,1] = np.cos((i+1)/(N/20))\n",
    "        Z[i,2] = 2 * ((i+1)/N-0.5)    \n",
    "    Z[:,3] = np.random.normal(0, 1, N)          \n",
    "    #Noise precisions\n",
    "    tau = [[] for _ in range(d.size)]\n",
    "    tau[0] = 5 * np.ones((1,d[0]))[0] \n",
    "    tau[1] = 10 * np.ones((1,d[1]))[0]\n",
    "    #ARD parameters\n",
    "    alpha = np.zeros((M, true_K))\n",
    "    alpha[0,:] = np.array([1,1,1e6,1])\n",
    "    alpha[1,:] = np.array([1,1,1,1e6])     \n",
    "    #W and X\n",
    "    W = [[] for _ in range(d.size)]\n",
    "    X_train = [[] for _ in range(d.size)]\n",
    "    X_test = [[] for _ in range(d.size)]\n",
    "    for i in range(0, d.size):\n",
    "        W[i] = np.zeros((d[i], true_K))\n",
    "        for t in range(0, true_K):\n",
    "            #generate W from p(W|alpha)\n",
    "            W[i][:,t] = np.random.normal(0, 1/np.sqrt(alpha[i,t]), d[i])\n",
    "        X = np.zeros((N, d[i]))\n",
    "        for j in range(0, d[i]):\n",
    "            #generate X from the generative model\n",
    "            X[:,j] = np.dot(Z,W[i][j,:].T) + \\\n",
    "            np.random.normal(0, 1/np.sqrt(tau[i][j]), N*1)    \n",
    "        #Get train and test data\n",
    "        X_train[i] = X[0:Ntrain,:] #Train data\n",
    "        X_test[i] = X[Ntrain:N,:] #Test data\n",
    "    #Latent variables for training the model    \n",
    "    Z = Z[0:Ntrain,:]\n",
    "\n",
    "    #Generate incomplete training data\n",
    "    if args.scenario == 'incomplete':\n",
    "        missing_Xtrue = [[] for _ in range(len(infoMiss['group']))]\n",
    "        for i in range(len(infoMiss['group'])):\n",
    "            g_miss = infoMiss['group'][i]-1  \n",
    "            if 'random' in infoMiss['type'][i]: \n",
    "                #remove entries randomly\n",
    "                missing_val =  np.random.choice([0, 1], \n",
    "                            size=(X_train[g_miss].shape[0],d[g_miss]), \n",
    "                            p=[1-infoMiss['perc'][i-1]/100, infoMiss['perc'][i-1]/100])\n",
    "                mask_miss =  np.ma.array(X_train[g_miss], mask = missing_val).mask\n",
    "                missing_Xtrue[i] = np.where(missing_val==1, X_train[g_miss],0)\n",
    "                X_train[g_miss][mask_miss] = 'NaN'\n",
    "            elif 'rows' in infoMiss['type'][i]: \n",
    "                #remove rows randomly\n",
    "                missing_Xtrue[i] = np.zeros((Ntrain,d[g_miss]))\n",
    "                n_rows = int(infoMiss['perc'][i-1]/100 * X_train[g_miss].shape[0])\n",
    "                shuf_samples = np.arange(Ntrain)\n",
    "                np.random.shuffle(shuf_samples)\n",
    "                missing_Xtrue[i][shuf_samples[0:n_rows],:] = X_train[g_miss][shuf_samples[0:n_rows],:]\n",
    "                X_train[g_miss][shuf_samples[0:n_rows],:] = 'NaN'\n",
    "    #Store data            \n",
    "    data = {'X_tr': X_train, 'X_te': X_test, 'W': W, 'Z': Z, 'tau': tau, 'alpha': alpha, 'true_K': true_K}\n",
    "    if args.scenario == 'incomplete':\n",
    "        data.update({'trueX_miss': missing_Xtrue}) \n",
    "    return data "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
