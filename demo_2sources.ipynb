{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import copy\n",
    "import argparse\n",
    "import visualization_syntdata\n",
    "from models import GFA_DiagonalNoiseModel, GFA_OriginalModel\n",
    "from utils import GFAtools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(description=\"GFA with two data sources\")\n",
    "    parser.add_argument(\"--scenario\", nargs='?', default='incomplete', type=str,\n",
    "                        help='Data scenario (complete or incomplete)')\n",
    "    parser.add_argument(\"--noise\", nargs='?', default='diagonal', type=str,\n",
    "                        help='Noise assumption for GFA models (diagonal or spherical)')\n",
    "    parser.add_argument(\"--num-sources\", nargs='?', default=2, type=int,\n",
    "                        help='Number of data sources')\n",
    "    parser.add_argument(\"--K\", nargs='?', default=6, type=int,\n",
    "                        help='number of components to initialised the model')\n",
    "    parser.add_argument(\"--impMedian\", nargs='?', default=True, type=bool,\n",
    "                        help='(not) impute median')\n",
    "    \n",
    "    args = parser.parse_args(\"\")                                                               \n",
    "    return args\t "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get arguments \n",
    "args = get_args() \n",
    "# Define parameters to generate incomplete data sets\n",
    "infoMiss = {'perc': [20,20], #percentage of missing data \n",
    "        'type': ['rows','random'], #type of missing data \n",
    "        'ds': [1,2]} #data sources that will have missing values        \n",
    "    \n",
    "# Make directory to save the results of the experiments         \n",
    "res_dir = f'results/{args.num_sources}dsources/GFA_{args.noise}/{args.K}comps/{args.scenario}'\n",
    "print(res_dir)\n",
    "if not os.path.exists(res_dir):\n",
    "        os.makedirs(res_dir)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_2g(args, infoMiss=None):\n",
    "\n",
    "    \"\"\" \n",
    "    Generate synthetic data with 2 data sources.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    args : local namespace \n",
    "        Arguments selected to run the model.\n",
    "\n",
    "    infoMiss : dict | None, optional.\n",
    "        Parameters selected to generate data with missing values.  \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : dict\n",
    "        Training and test data as well as model parameters used \n",
    "        to generate the data.\n",
    "    \n",
    "    \"\"\"\n",
    "    Ntrain = 400; Ntest = 100\n",
    "    N = Ntrain + Ntest #  total number of samples\n",
    "    M = args.num_sources  #number of data sources\n",
    "    d = np.array([50, 30]) #number of dimensios in each data source\n",
    "    true_K = 4  # true latent components\n",
    "    # Specify Z manually\n",
    "    Z = np.zeros((N, true_K))\n",
    "    for i in range(0, N):\n",
    "        Z[i,0] = np.sin((i+1)/(N/20))\n",
    "        Z[i,1] = np.cos((i+1)/(N/20))\n",
    "        Z[i,2] = 2 * ((i+1)/N-0.5)    \n",
    "    Z[:,3] = np.random.normal(0, 1, N)          \n",
    "    # Specify noise precisions manually\n",
    "    tau = [[] for _ in range(d.size)]\n",
    "    tau[0] = 5 * np.ones((1,d[0]))[0] \n",
    "    tau[1] = 10 * np.ones((1,d[1]))[0]\n",
    "    # Specify alphas manually\n",
    "    alpha = np.zeros((M, true_K))\n",
    "    alpha[0,:] = np.array([1,1,1e6,1])\n",
    "    alpha[1,:] = np.array([1,1,1,1e6])     \n",
    "    \n",
    "    #W and X\n",
    "    W = [[] for _ in range(d.size)]\n",
    "    X_train = [[] for _ in range(d.size)]\n",
    "    X_test = [[] for _ in range(d.size)]\n",
    "    for i in range(0, d.size):\n",
    "        W[i] = np.zeros((d[i], true_K))\n",
    "        for t in range(0, true_K):\n",
    "            #generate W from p(W|alpha)\n",
    "            W[i][:,t] = np.random.normal(0, 1/np.sqrt(alpha[i,t]), d[i])\n",
    "        X = np.zeros((N, d[i]))\n",
    "        for j in range(0, d[i]):\n",
    "            #generate X from the generative model\n",
    "            X[:,j] = np.dot(Z,W[i][j,:].T) + \\\n",
    "            np.random.normal(0, 1/np.sqrt(tau[i][j]), N*1)    \n",
    "        # Get training and test data\n",
    "        X_train[i] = X[0:Ntrain,:] #Training data\n",
    "        X_test[i] = X[Ntrain:N,:] #Test data\n",
    "    #latent variables for training the model    \n",
    "    Z = Z[0:Ntrain,:]\n",
    "\n",
    "    # Generate incomplete training data\n",
    "    if args.scenario == 'incomplete':\n",
    "        X_train, missing_Xtrue = generate_missdata(X_train, infoMiss)\n",
    "    \n",
    "    # Store data and model parameters            \n",
    "    data = {'X_tr': X_train, 'X_te': X_test, 'W': W, 'Z': Z, 'tau': tau, 'alpha': alpha, 'true_K': true_K}\n",
    "    if args.scenario == 'incomplete':\n",
    "        data.update({'trueX_miss': missing_Xtrue}) \n",
    "    return data \n",
    "\n",
    "def generate_missdata(X_train, infoMiss):\n",
    "    \"\"\" \n",
    "    Generate missing data in the training data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : list \n",
    "        List of arrays containing the data matrix of each data \n",
    "        source.\n",
    "\n",
    "    infoMiss : dict \n",
    "        Parameters selected to generate data with missing values.  \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_miss : list \n",
    "        List of arrays containing the training data. The data \n",
    "        sources specified in infoMiss will have missing values.\n",
    "\n",
    "    missing_Xtrue : list \n",
    "        List of arrays containing the true values removed from the\n",
    "        data sources selected in infoMiss.     \n",
    "    \n",
    "    \"\"\"\n",
    "    missing_Xtrue = [[] for _ in range(len(infoMiss['ds']))]\n",
    "    for i in range(len(infoMiss['ds'])):\n",
    "        g_miss = infoMiss['ds'][i]-1  \n",
    "        if 'random' in infoMiss['type'][i]: \n",
    "            #remove entries randomly\n",
    "            missing_val =  np.random.choice([0, 1], \n",
    "                        size=(X_train[g_miss].shape[0],X_train[g_miss].shape[1]), \n",
    "                        p=[1-infoMiss['perc'][i-1]/100, infoMiss['perc'][i-1]/100])\n",
    "            mask_miss =  np.ma.array(X_train[g_miss], mask = missing_val).mask\n",
    "            missing_Xtrue[i] = np.where(missing_val==1, X_train[g_miss],0)\n",
    "            X_train[g_miss][mask_miss] = 'NaN'\n",
    "        elif 'rows' in infoMiss['type'][i]: \n",
    "            #remove rows randomly\n",
    "            Ntrain = X_train[g_miss].shape[0]\n",
    "            missing_Xtrue[i] = np.zeros((Ntrain, X_train[g_miss].shape[1]))\n",
    "            n_rows = int(infoMiss['perc'][i-1]/100 * Ntrain)\n",
    "            shuf_samples = np.arange(Ntrain)\n",
    "            np.random.shuffle(shuf_samples)\n",
    "            missing_Xtrue[i][shuf_samples[0:n_rows],:] = X_train[g_miss][shuf_samples[0:n_rows],:]\n",
    "            X_train[g_miss][shuf_samples[0:n_rows],:] = 'NaN'\n",
    "        X_miss = X_train\n",
    "    return X_miss, missing_Xtrue \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "if args.scenario == 'complete':\n",
    "    data = get_data_2g(args)\n",
    "else:\n",
    "    data = get_data_2g(args, infoMiss)\n",
    "\n",
    "#save file with generated data\n",
    "data_file = f'{res_dir}/[1]Data.dictionary'\n",
    "with open(data_file, 'wb') as parameters:\n",
    "        pickle.dump(data, parameters)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = data['X_tr'] #get training data\n",
    "# Initialise the model\n",
    "if 'diagonal' in args.noise:    \n",
    "    GFAmodel = GFA_DiagonalNoiseModel(X_tr, args)\n",
    "else:\n",
    "    assert args.scenario == 'complete'\n",
    "    GFAmodel = GFA_OriginalModel(X_tr, args) \n",
    "    \n",
    "#Fit the model\n",
    "time_start = time.process_time()\n",
    "GFAmodel.fit(X_tr)\n",
    "GFAmodel.time_elapsed = time.process_time() - time_start\n",
    "print(f'Computational time: {float(\"{:.2f}\".format(GFAmodel.time_elapsed))}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions (Predict source 2 from source 1) \n",
    "# Compute mean squared error (MSE)\n",
    "obs_ds = np.array([1, 0]) #data source 1 was observed\n",
    "gpred = np.where(obs_ds == 0)[0][0] #get the non-observed data source\n",
    "X_test = data['X_te']\n",
    "X_pred = GFAtools(X_test, GFAmodel).PredictDSources(obs_ds, args.noise)\n",
    "\n",
    "#Compute MSE \n",
    "GFAmodel.MSE = np.mean((X_test[gpred] - X_pred[0]) ** 2)\n",
    "print(f'MSE: {float(\"{:.2f}\".format(GFAmodel.MSE))}')\n",
    "\n",
    "# Compute MSE - chance level (MSE between test values and train means)\n",
    "Tr_means = np.ones((X_test[gpred].shape[0], X_test[gpred].shape[1])) * \\\n",
    "    np.nanmean(data['X_tr'][gpred], axis=0)           \n",
    "GFAmodel.MSE_chlev = np.mean((X_test[gpred] - Tr_means) ** 2)\n",
    "print(f'MSE (chance level): {float(\"{:.2f}\".format(GFAmodel.MSE_chlev))}')\n",
    "\n",
    "# Save file containing model outputs and predictions\n",
    "res_file = f'{res_dir}/[1]ModelOutput.dictionary'\n",
    "with open(res_file, 'wb') as parameters:\n",
    "    pickle.dump(GFAmodel, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.impMedian: \n",
    "    assert args.scenario == 'incomplete'\n",
    "    X_impmed = copy.deepcopy(data['X_tr'])\n",
    "    g_miss = np.array(infoMiss['ds']) - 1 #data source with missing values  \n",
    "    # Impute median before training the model\n",
    "    for i in range(g_miss.size):\n",
    "        for j in range(data['X_tr'][g_miss[i]].shape[1]):\n",
    "            Xtrain_j = data['X_tr'][g_miss[i]][:,j]\n",
    "            X_impmed[g_miss[i]][np.isnan(X_impmed[g_miss[i]][:,j]),j] = np.nanmedian(Xtrain_j)\n",
    "\n",
    "    # Initialise the model\n",
    "    GFAmodel_median = GFA_DiagonalNoiseModel(X_impmed, args, imputation=True)\n",
    "    # Fit the model\n",
    "    time_start = time.process_time()\n",
    "    GFAmodel_median.fit(X_impmed)\n",
    "    GFAmodel_median.time_elapsed = time.process_time() - time_start\n",
    "    print(f'Computational time: {float(\"{:.2f}\".format(GFAmodel_median.time_elapsed))}s')\n",
    "\n",
    "    # Predictions (Predict source 2 from source 1) \n",
    "    obs_ds = np.array([1, 0]) #data source 1 was observed \n",
    "    gpred = np.where(obs_ds == 0)[0][0] #get the non-observed data source\n",
    "    X_test = data['X_te']\n",
    "    X_pred = GFAtools(X_test, GFAmodel_median).PredictDSources(obs_ds, args.noise)\n",
    "    GFAmodel_median.MSE = np.mean((X_test[gpred] - X_pred[0]) ** 2) \n",
    "\n",
    "    # Save file containing model outputs and predictions\n",
    "    res_med_file = f'{res_dir}/[1]ModelOutput_median.dictionary'\n",
    "    with open(res_med_file, 'wb') as parameters:\n",
    "        pickle.dump(GFAmodel_median, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import auxiliary functions for visualization\n",
    "from visualization_syntdata import hinton_diag, match_factors, plot_loadings, plot_Z\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model_1 = GFAmodel #model without imputation\n",
    "model_2 = GFAmodel_median #model with median imputation\n",
    "\n",
    "#Concatenate (true and estimated) loadings and alphas across data sources\n",
    "#model_1\n",
    "W_est_1 = np.zeros((np.sum(model_1.d),model_1.k))\n",
    "alphas_est_1 = np.zeros((model_1.k, args.num_sources))\n",
    "#model_2\n",
    "W_est_2 = np.zeros((np.sum(model_2.d),model_2.k))\n",
    "alphas_est_2 = np.zeros((model_2.k, args.num_sources))\n",
    "#true parameters\n",
    "W_true = np.zeros((np.sum(model_1.d),data['true_K']))\n",
    "alphas_true = np.zeros((data['true_K'], args.num_sources))\n",
    "d = 0\n",
    "for m in range(args.num_sources):\n",
    "    Dm = model_1.d[m]\n",
    "    alphas_true[:,m] = data['alpha'][m]\n",
    "    W_true[d:d+Dm,:] = data['W'][m]\n",
    "    #model_1\n",
    "    alphas_est_1[:,m] = model_1.E_alpha[m]\n",
    "    W_est_1[d:d+Dm,:] = model_1.means_w[m]\n",
    "    #model_2\n",
    "    alphas_est_2[:,m] = model_2.E_alpha[m]\n",
    "    W_est_2[d:d+Dm,:] = model_2.means_w[m]\n",
    "    d += Dm  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loading matrices\n",
    "#------------------------------------------------------------------\n",
    "#plot true Ws\n",
    "W_path = f'{res_dir}/[1]W_true.png'\n",
    "plot_loadings(W_true, model_1.d, W_path) \n",
    "\n",
    "#plot estimated Ws - model 1\n",
    "if model_1.k == data['true_K']:\n",
    "    #match true and estimated components\n",
    "    W_est_1, sim_factors_1, flip_1 = match_factors(W_est_1, W_true)\n",
    "W_path_1 = f'{res_dir}/[1]W_est.png' \n",
    "plot_loadings(W_est_1, model_1.d, W_path_1)\n",
    "\n",
    "#plot estimated Ws - model 2\n",
    "if model_2.k == data['true_K']:\n",
    "    #match true and estimated components\n",
    "    W_est_2, sim_factors_2, flip_1 = match_factors(W_est_2, W_true)\n",
    "W_path_2 = f'{res_dir}/[1]W_est_median.png' \n",
    "plot_loadings(W_est_2, model_2.d, W_path_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot latent variables\n",
    "#------------------------------------------------------------------------------------ \n",
    "#plot true latent variables \n",
    "Z_path = f'{res_dir}/[1]Z_true.png'    \n",
    "plot_Z(data['Z'], Z_path)\n",
    "\n",
    "#plot estimated latent variables - model 1\n",
    "Z_path_1 = f'{res_dir}/[1]Z_est.png'\n",
    "if model_1.k == data['true_K']:\n",
    "    plot_Z(model_1.means_z, Z_path_1, match=True, s_comps=sim_factors_1, flip=flip_1)\n",
    "else:     \n",
    "    plot_Z(model_1.means_z, Z_path_1)    \n",
    "    \n",
    "#plot estimated latent variables - model 2\n",
    "Z_path_2 = f'{res_dir}/[1]Z_est_median.png'\n",
    "if model_2.k == data['true_K']:\n",
    "    plot_Z(model_2.means_z, Z_path_2, match=True, s_comps=sim_factors_2, flip=flip_2)\n",
    "else:     \n",
    "    plot_Z(model_2.means_z, Z_path_2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot alphas\n",
    "#---------------------------------------------------------------------------\n",
    "#plot true alphas\n",
    "alphas_path = f'{res_dir}/[1]alphas_true.png'\n",
    "hinton_diag(np.negative(alphas_true.T), alphas_path)     \n",
    "\n",
    "#plot estimated alphas - model 1\n",
    "alphas_path_1 = f'{res_dir}/[1]alphas_est.png'\n",
    "if model_1.k == data['true_K']:\n",
    "    hinton_diag(np.negative(alphas_est_1[sim_factors_1,:].T), alphas_path_1) \n",
    "else:\n",
    "    hinton_diag(np.negative(alphas_est_1.T), alphas_path_1)\n",
    "\n",
    "#plot estimated alphas - model 2\n",
    "alphas_path_2 = f'{res_dir}/[1]alphas_est_median.png'\n",
    "if model_2.k == data['true_K']:\n",
    "    hinton_diag(np.negative(alphas_est_2[sim_factors_1,:].T), alphas_path_2) \n",
    "else:\n",
    "    hinton_diag(np.negative(alphas_est_2.T), alphas_path_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ELBO - model 1\n",
    "L_path_1 = f'{res_dir}/[1]ELBO.png'    \n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.plot(model_1.L[1:])\n",
    "plt.savefig(L_path_1)\n",
    "plt.close() \n",
    "\n",
    "# Plot ELBO - model 2\n",
    "L_path_2 = f'{res_dir}/[1]ELBO_median.png'\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.plot(model_2.L[1:])\n",
    "plt.savefig(L_path_2)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree(res_dir, ignore_errors=True)\n",
    "\n",
    "%run -i 'analysis_syntdata.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
